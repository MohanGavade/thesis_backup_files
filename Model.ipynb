{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Lambda, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.backend as K\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Columns:\n",
      " Index(['timestamp', 'rcs', 'distance', 'angleAzimuth', 'angleElevation',\n",
      "       'x_det', 'y_det', 'z', 'radialVelocity', 'objectID',\n",
      "       'radialVelocityDomainMax', 'orientation', 'x_obj', 'y_obj',\n",
      "       'width_edge_mean', 'length_edge_mean', 'object_id', 'header.origin.x',\n",
      "       'header.origin.y', 'header.origin.z', 'header.origin.roll',\n",
      "       'header.origin.pitch', 'header.origin.yaw', 'reference_point',\n",
      "       'yaw_rate', 'ego_speed', 'centroid_x', 'centroid_y',\n",
      "       'is_valid_detection'],\n",
      "      dtype='object')\n",
      "\n",
      "First 5 Rows:\n",
      "       timestamp                                                rcs  \\\n",
      "0  2.746754e+07  [-18 -32 -25 -21 -32 -29 -36 -37 -31 -19  -5 -...   \n",
      "1  2.746755e+07  [-23 -23 -22  -7 -28 -39 -25 -26 -23 -26 -27 -...   \n",
      "2  2.746755e+07  [-35 -28 -25 -26 -35 -40 -28 -24 -32 -32 -25 -...   \n",
      "3  2.746755e+07  [-34 -23 -24 -27 -33 -34 -24 -26 -31 -24 -38 -...   \n",
      "4  2.746755e+07  [-28 -28 -26 -28 -26 -26 -30 -26 -31 -25 -26 -...   \n",
      "\n",
      "                                            distance  \\\n",
      "0  [  2.6799998   3.12        3.1599998   3.21   ...   \n",
      "1  [  2.6299999   2.6799998   2.7         3.01   ...   \n",
      "2  [  3.22        3.28        3.29        3.3    ...   \n",
      "3  [  3.12        3.12        3.25        3.26   ...   \n",
      "4  [  3.26        3.28        3.31        3.39999...   \n",
      "\n",
      "                                        angleAzimuth  \\\n",
      "0  [-0.9527408  -0.5877923  -0.8913226  -0.933158...   \n",
      "1  [-1.0275631e+00 -1.0343698e+00 -9.2038232e-01 ...   \n",
      "2  [-4.33539972e-02 -6.87694967e-01 -7.36127853e-...   \n",
      "3  [-0.76838154 -0.80702317 -0.9909111  -0.632193...   \n",
      "4  [-7.39269435e-01 -7.66234815e-01 -6.75547421e-...   \n",
      "\n",
      "                                      angleElevation  \\\n",
      "0  [-0.2622184  -0.20818296 -0.13194695 -0.153152...   \n",
      "1  [-0.1875008  -0.18336438 -0.1625775   0.357775...   \n",
      "2  [-0.21032973 -0.15666083 -0.14074342 -0.140638...   \n",
      "3  [-0.15896466 -0.14927807 -0.11613426 -0.194202...   \n",
      "4  [-0.15440935 -0.14425153 -0.19477883 -0.140062...   \n",
      "\n",
      "                                               x_det  \\\n",
      "0  [  5.0138464   6.054301    5.4824324   5.40254...   \n",
      "1  [  4.849637    4.8607      5.1273346   5.11493...   \n",
      "2  [  6.6600795   6.017455    5.9280252   5.95312...   \n",
      "3  [  5.7291083   5.6479535   5.2827706   6.09451...   \n",
      "4  [  5.894352    5.8527813   6.0481644   6.14114...   \n",
      "\n",
      "                                               y_det  \\\n",
      "0  [-2.4095554e+00 -1.9927621e+00 -2.7368078e+00 ...   \n",
      "1  [-2.51193047e+00 -2.56495070e+00 -2.42041469e+...   \n",
      "2  [-4.36480641e-01 -2.35650873e+00 -2.48714042e+...   \n",
      "3  [-2.44097495e+00 -2.52830172e+00 -3.00039482e+...   \n",
      "4  [-2.47028732e+00 -2.55081868e+00 -2.33068514e+...   \n",
      "\n",
      "                                                   z  \\\n",
      "0  [-1.57195330e-02  3.41508389e-02  2.63256460e-...   \n",
      "1  [ 1.88757271e-01  1.90332651e-01  2.41971940e-...   \n",
      "2  [ 6.72078133e-03  1.67251766e-01  2.17481405e-...   \n",
      "3  [ 1.85116500e-01  2.14980304e-01  3.02411497e-...   \n",
      "4  [ 1.77623391e-01  2.07494229e-01  3.83509994e-...   \n",
      "\n",
      "                                      radialVelocity  \\\n",
      "0  [-2.1499999  -2.07       -1.5949999  -1.475   ...   \n",
      "1  [-2.4800000e+00 -2.4649999e+00 -3.0400000e+00 ...   \n",
      "2  [-5.0749998e+00 -3.9549999e+00 -3.8299999e+00 ...   \n",
      "3  [-3.6650000e+00 -3.5200000e+00 -2.7749999e+00 ...   \n",
      "4  [ -3.7849998  -3.6599998  -3.9299998  -4.02   ...   \n",
      "\n",
      "                                            objectID  ...  header.origin.z  \\\n",
      "0  [   0    0 3084 3084    0    0    0    0    0 ...  ...            0.679   \n",
      "1  [   0    0    0    0    0    0    0    0    0 ...  ...            0.679   \n",
      "2  [   0    0    0    0    0    0    0    0    0 ...  ...            0.679   \n",
      "3  [   0    0    0    0    0    0    0    0    0 ...  ...            0.679   \n",
      "4  [   0    0    0    0    0    0    0    0    0 ...  ...            0.679   \n",
      "\n",
      "  header.origin.roll header.origin.pitch header.origin.yaw  \\\n",
      "0                0.0                 0.0               0.0   \n",
      "1                0.0                 0.0               0.0   \n",
      "2                0.0                 0.0               0.0   \n",
      "3                0.0                 0.0               0.0   \n",
      "4                0.0                 0.0               0.0   \n",
      "\n",
      "                                     reference_point  yaw_rate  ego_speed  \\\n",
      "0  [(3, 'Middle_Side_Right'), (5, 'Middle_Rear'),... -0.975006   9.703125   \n",
      "1  [(1, 'Middle_Front'), (3, 'Middle_Side_Right')...  0.425003  20.734375   \n",
      "2  [(1, 'Middle_Front'), (3, 'Middle_Side_Right')...  0.524994  20.671875   \n",
      "3  [(1, 'Middle_Front'), (1, 'Middle_Front'), (3,...  0.384995  20.609375   \n",
      "4  [(7, 'Middle_Side_Left'), (3, 'Middle_Side_Rig...  0.365005  20.468750   \n",
      "\n",
      "             centroid_x            centroid_y  \\\n",
      "0  [103.43499755859376]  [-1.814999938011169]   \n",
      "1  [52.509998321533196]  [2.4099998474121134]   \n",
      "2  [52.014997482299805]  [2.4199999570846584]   \n",
      "3   [51.73499870300293]  [2.3999999761581425]   \n",
      "4  [51.315000534057624]  [2.5499998927116376]   \n",
      "\n",
      "                                  is_valid_detection  \n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define file path\n",
    "csv_path = \"/home/q674749/workspace/thesis_work/rat25-15.4.1/perception/BERT_NOTEBOOK/final_leading_object_data/combined_file.csv\"\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Display column names and first few rows\n",
    "print(\"Dataset Columns:\\n\", df.columns)\n",
    "print(\"\\nFirst 5 Rows:\\n\", df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of detections per row: 995\n",
      "\n",
      "Sample Processed Data:\n",
      "       timestamp                                                rcs  \\\n",
      "0  2.746754e+07  [-18.0, -32.0, -25.0, -21.0, -32.0, -29.0, -36...   \n",
      "1  2.746755e+07  [-23.0, -23.0, -22.0, -7.0, -28.0, -39.0, -25....   \n",
      "2  2.746755e+07  [-35.0, -28.0, -25.0, -26.0, -35.0, -40.0, -28...   \n",
      "3  2.746755e+07  [-34.0, -23.0, -24.0, -27.0, -33.0, -34.0, -24...   \n",
      "4  2.746755e+07  [-28.0, -28.0, -26.0, -28.0, -26.0, -26.0, -30...   \n",
      "\n",
      "                                            distance  \\\n",
      "0  [  2.6799998   3.12        3.1599998   3.21   ...   \n",
      "1  [  2.6299999   2.6799998   2.7         3.01   ...   \n",
      "2  [  3.22        3.28        3.29        3.3    ...   \n",
      "3  [  3.12        3.12        3.25        3.26   ...   \n",
      "4  [  3.26        3.28        3.31        3.39999...   \n",
      "\n",
      "                                        angleAzimuth  \\\n",
      "0  [-0.9527408  -0.5877923  -0.8913226  -0.933158...   \n",
      "1  [-1.0275631e+00 -1.0343698e+00 -9.2038232e-01 ...   \n",
      "2  [-4.33539972e-02 -6.87694967e-01 -7.36127853e-...   \n",
      "3  [-0.76838154 -0.80702317 -0.9909111  -0.632193...   \n",
      "4  [-7.39269435e-01 -7.66234815e-01 -6.75547421e-...   \n",
      "\n",
      "                                      angleElevation  \\\n",
      "0  [-0.2622184  -0.20818296 -0.13194695 -0.153152...   \n",
      "1  [-0.1875008  -0.18336438 -0.1625775   0.357775...   \n",
      "2  [-0.21032973 -0.15666083 -0.14074342 -0.140638...   \n",
      "3  [-0.15896466 -0.14927807 -0.11613426 -0.194202...   \n",
      "4  [-0.15440935 -0.14425153 -0.19477883 -0.140062...   \n",
      "\n",
      "                                               x_det  \\\n",
      "0  [5.0138464, 6.054301, 5.4824324, 5.402544, 6.6...   \n",
      "1  [4.849637, 4.8607, 5.1273346, 5.1149383, 5.930...   \n",
      "2  [6.6600795, 6.017455, 5.9280252, 5.953123, 6.7...   \n",
      "3  [5.7291083, 5.6479535, 5.2827706, 6.0945125, 6...   \n",
      "4  [5.894352, 5.8527813, 6.0481644, 6.1411476, 6....   \n",
      "\n",
      "                                               y_det  \\\n",
      "0  [-2.4095554, -1.9927621, -2.7368078, -2.849057...   \n",
      "1  [-2.51193047, -2.5649507, -2.42041469, -2.6207...   \n",
      "2  [-0.436480641, -2.35650873, -2.48714042, -2.47...   \n",
      "3  [-2.44097495, -2.52830172, -3.00039482, -2.190...   \n",
      "4  [-2.47028732, -2.55081868, -2.33068514, -2.405...   \n",
      "\n",
      "                                                   z  \\\n",
      "0  [-1.57195330e-02  3.41508389e-02  2.63256460e-...   \n",
      "1  [ 1.88757271e-01  1.90332651e-01  2.41971940e-...   \n",
      "2  [ 6.72078133e-03  1.67251766e-01  2.17481405e-...   \n",
      "3  [ 1.85116500e-01  2.14980304e-01  3.02411497e-...   \n",
      "4  [ 1.77623391e-01  2.07494229e-01  3.83509994e-...   \n",
      "\n",
      "                                      radialVelocity  \\\n",
      "0  [-2.1499999, -2.07, -1.5949999, -1.475, -2.215...   \n",
      "1  [-2.48, -2.4649999, -3.04, -2.8299999, -3.8099...   \n",
      "2  [-5.0749998, -3.9549999, -3.8299999, -3.834999...   \n",
      "3  [-3.665, -3.52, -2.7749999, -4.1100001, -4.614...   \n",
      "4  [-3.7849998, -3.6599998, -3.9299998, -4.02, -4...   \n",
      "\n",
      "                                            objectID  ...  header.origin.z  \\\n",
      "0  [   0    0 3084 3084    0    0    0    0    0 ...  ...            0.679   \n",
      "1  [   0    0    0    0    0    0    0    0    0 ...  ...            0.679   \n",
      "2  [   0    0    0    0    0    0    0    0    0 ...  ...            0.679   \n",
      "3  [   0    0    0    0    0    0    0    0    0 ...  ...            0.679   \n",
      "4  [   0    0    0    0    0    0    0    0    0 ...  ...            0.679   \n",
      "\n",
      "   header.origin.roll header.origin.pitch header.origin.yaw  \\\n",
      "0                 0.0                 0.0               0.0   \n",
      "1                 0.0                 0.0               0.0   \n",
      "2                 0.0                 0.0               0.0   \n",
      "3                 0.0                 0.0               0.0   \n",
      "4                 0.0                 0.0               0.0   \n",
      "\n",
      "                                     reference_point  yaw_rate  ego_speed  \\\n",
      "0  [(3, 'Middle_Side_Right'), (5, 'Middle_Rear'),... -0.975006   9.703125   \n",
      "1  [(1, 'Middle_Front'), (3, 'Middle_Side_Right')...  0.425003  20.734375   \n",
      "2  [(1, 'Middle_Front'), (3, 'Middle_Side_Right')...  0.524994  20.671875   \n",
      "3  [(1, 'Middle_Front'), (1, 'Middle_Front'), (3,...  0.384995  20.609375   \n",
      "4  [(7, 'Middle_Side_Left'), (3, 'Middle_Side_Rig...  0.365005  20.468750   \n",
      "\n",
      "   centroid_x  centroid_y                                 is_valid_detection  \n",
      "0  103.434998      -1.815  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
      "1   52.509998       2.410  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2   52.014997       2.420  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3   51.734999       2.400  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "4   51.315001       2.550  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "\n",
      "[5 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Convert string representations of lists to actual lists if needed\n",
    "def convert_to_list(value):\n",
    "    \"\"\"Ensure values are lists. If string, convert to list of floats.\"\"\"\n",
    "    if isinstance(value, list):  # Already a list, return as-is\n",
    "        return value\n",
    "    elif isinstance(value, str):  # Convert string to list\n",
    "        return [float(x) for x in value.replace(\"[\", \"\").replace(\"]\", \"\").split()]\n",
    "    else:\n",
    "        return []  # Return empty list if unexpected format\n",
    "\n",
    "# Apply conversion to detection columns\n",
    "for col in [\"x_det\", \"y_det\", \"radialVelocity\", \"rcs\"]:\n",
    "    df[col] = df[col].apply(convert_to_list)\n",
    "\n",
    "# Convert label columns (single value per row)\n",
    "def extract_single_value(value):\n",
    "    \"\"\"Extract a single float from a list or string representation\"\"\"\n",
    "    if isinstance(value, list):  \n",
    "        return float(value[0])  # Extract first value if already list\n",
    "    elif isinstance(value, str):  \n",
    "        return float(value.replace(\"[\", \"\").replace(\"]\", \"\"))  # Remove brackets and convert\n",
    "    else:\n",
    "        return float(value)  # Convert directly if already a float\n",
    "\n",
    "# Apply conversion to label columns\n",
    "for col in [\"orientation\", \"width_edge_mean\", \"length_edge_mean\", \"centroid_x\", \"centroid_y\"]:\n",
    "    df[col] = df[col].apply(extract_single_value)\n",
    "\n",
    "# Ensure scalar feature is float\n",
    "df[\"ego_speed\"] = df[\"ego_speed\"].astype(float)\n",
    "\n",
    "# Find maximum detections across all rows\n",
    "max_detections = max(df[\"x_det\"].apply(len).max(),\n",
    "                     df[\"y_det\"].apply(len).max(),\n",
    "                     df[\"radialVelocity\"].apply(len).max(),\n",
    "                     df[\"rcs\"].apply(len).max())\n",
    "\n",
    "print(f\"Maximum number of detections per row: {max_detections}\")\n",
    "print(\"\\nSample Processed Data:\\n\", df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved as: /home/q674749/workspace/thesis_work/rat25-15.4.1/perception/BERT_NOTEBOOK/final_leading_object_data/mohan.csv\n",
      "\n",
      "Sample Data:\n",
      "                                                x_det  \\\n",
      "0  [5.0138464, 6.054301, 5.4824324, 5.402544, 6.6...   \n",
      "1  [4.849637, 4.8607, 5.1273346, 5.1149383, 5.930...   \n",
      "2  [6.6600795, 6.017455, 5.9280252, 5.953123, 6.7...   \n",
      "3  [5.7291083, 5.6479535, 5.2827706, 6.0945125, 6...   \n",
      "4  [5.894352, 5.8527813, 6.0481644, 6.1411476, 6....   \n",
      "\n",
      "                                               y_det  \\\n",
      "0  [-2.4095554, -1.9927621, -2.7368078, -2.849057...   \n",
      "1  [-2.51193047, -2.5649507, -2.42041469, -2.6207...   \n",
      "2  [-0.436480641, -2.35650873, -2.48714042, -2.47...   \n",
      "3  [-2.44097495, -2.52830172, -3.00039482, -2.190...   \n",
      "4  [-2.47028732, -2.55081868, -2.33068514, -2.405...   \n",
      "\n",
      "                                      radialVelocity  \\\n",
      "0  [-2.1499999, -2.07, -1.5949999, -1.475, -2.215...   \n",
      "1  [-2.48, -2.4649999, -3.04, -2.8299999, -3.8099...   \n",
      "2  [-5.0749998, -3.9549999, -3.8299999, -3.834999...   \n",
      "3  [-3.665, -3.52, -2.7749999, -4.1100001, -4.614...   \n",
      "4  [-3.7849998, -3.6599998, -3.9299998, -4.02, -4...   \n",
      "\n",
      "                                                 rcs  ego_speed  orientation  \\\n",
      "0  [-18.0, -32.0, -25.0, -21.0, -32.0, -29.0, -36...   9.703125     0.062416   \n",
      "1  [-23.0, -23.0, -22.0, -7.0, -28.0, -39.0, -25....  20.734375     2.324243   \n",
      "2  [-35.0, -28.0, -25.0, -26.0, -35.0, -40.0, -28...  20.671875    -0.356086   \n",
      "3  [-34.0, -23.0, -24.0, -27.0, -33.0, -34.0, -24...  20.609375     3.141304   \n",
      "4  [-28.0, -28.0, -26.0, -28.0, -26.0, -26.0, -30...  20.468750    -0.296738   \n",
      "\n",
      "   width_edge_mean  length_edge_mean  centroid_x  centroid_y  \n",
      "0             0.25              0.25  103.434998      -1.815  \n",
      "1             1.20              0.68   52.509998       2.410  \n",
      "2             1.20              0.69   52.014997       2.420  \n",
      "3             1.20              0.81   51.734999       2.400  \n",
      "4             1.20              0.77   51.315001       2.550  \n"
     ]
    }
   ],
   "source": [
    "# Define relevant columns\n",
    "relevant_columns = [\"x_det\", \"y_det\", \"radialVelocity\", \"rcs\", \n",
    "                    \"ego_speed\", \n",
    "                    \"orientation\", \"width_edge_mean\", \"length_edge_mean\", \"centroid_x\", \"centroid_y\"]\n",
    "\n",
    "# Keep only relevant columns\n",
    "df_cleaned = df[relevant_columns]\n",
    "\n",
    "# Define the new file path\n",
    "new_csv_path = \"/home/q674749/workspace/thesis_work/rat25-15.4.1/perception/BERT_NOTEBOOK/final_leading_object_data/mohan.csv\"\n",
    "\n",
    "# Save cleaned data\n",
    "df_cleaned.to_csv(new_csv_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved as: {new_csv_path}\")\n",
    "print(\"\\nSample Data:\\n\", df_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: 27155 rows saved to /home/q674749/workspace/thesis_work/rat25-15.4.1/perception/BERT_NOTEBOOK/final_leading_object_data/mohan_train.csv\n",
      "Validation Set: 5819 rows saved to /home/q674749/workspace/thesis_work/rat25-15.4.1/perception/BERT_NOTEBOOK/final_leading_object_data/mohan_val.csv\n",
      "Test Set: 5820 rows saved to /home/q674749/workspace/thesis_work/rat25-15.4.1/perception/BERT_NOTEBOOK/final_leading_object_data/mohan_test.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the cleaned dataset\n",
    "csv_path = \"/home/q674749/workspace/thesis_work/rat25-15.4.1/perception/BERT_NOTEBOOK/final_leading_object_data/mohan.csv\"\n",
    "df_cleaned = pd.read_csv(csv_path)\n",
    "\n",
    "# Split into 70% Train, 15% Validation, 15% Test\n",
    "train_df, temp_df = train_test_split(df_cleaned, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save splits as new CSV files\n",
    "train_path = \"/home/q674749/workspace/thesis_work/rat25-15.4.1/perception/BERT_NOTEBOOK/final_leading_object_data/mohan_train.csv\"\n",
    "val_path = \"/home/q674749/workspace/thesis_work/rat25-15.4.1/perception/BERT_NOTEBOOK/final_leading_object_data/mohan_val.csv\"\n",
    "test_path = \"/home/q674749/workspace/thesis_work/rat25-15.4.1/perception/BERT_NOTEBOOK/final_leading_object_data/mohan_test.csv\"\n",
    "\n",
    "train_df.to_csv(train_path, index=False)\n",
    "val_df.to_csv(val_path, index=False)\n",
    "test_df.to_csv(test_path, index=False)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Training Set: {len(train_df)} rows saved to {train_path}\")\n",
    "print(f\"Validation Set: {len(val_df)} rows saved to {val_path}\")\n",
    "print(f\"Test Set: {len(test_df)} rows saved to {test_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Input features and labels successfully converted and saved for train, validation, and test sets!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load datasets\n",
    "train_path = \"/home/q674749/workspace/thesis_work/rat25-15.4.1/perception/BERT_NOTEBOOK/final_leading_object_data/mohan_train.csv\"\n",
    "val_path = \"/home/q674749/workspace/thesis_work/rat25-15.4.1/perception/BERT_NOTEBOOK/final_leading_object_data/mohan_val.csv\"\n",
    "test_path = \"/home/q674749/workspace/thesis_work/rat25-15.4.1/perception/BERT_NOTEBOOK/final_leading_object_data/mohan_test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "val_df = pd.read_csv(val_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Convert detection features (list-based columns)\n",
    "def convert_to_list(value):\n",
    "    \"\"\"Ensure values are lists. If string, clean and convert to list of floats.\"\"\"\n",
    "    if isinstance(value, list):  \n",
    "        return value  # Already a list\n",
    "    elif isinstance(value, str):  \n",
    "        value = value.replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \" \")  # Clean brackets and commas\n",
    "        return [float(x) for x in value.split() if x.strip()]  # Convert to floats\n",
    "    else:\n",
    "        return []  # Return empty list if unexpected format\n",
    "\n",
    "# Convert label columns (single float per row)\n",
    "def extract_single_value(value):\n",
    "    \"\"\"Extract a single float from a list or string representation.\"\"\"\n",
    "    if isinstance(value, list):  \n",
    "        return float(value[0])  # Extract first value if already a list\n",
    "    elif isinstance(value, str):  \n",
    "        return float(value.replace(\"[\", \"\").replace(\"]\", \"\"))  # Remove brackets and convert\n",
    "    else:\n",
    "        return float(value)  # Convert directly if already a float\n",
    "\n",
    "# Apply to all datasets (train, validation, test)\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    # Apply conversion to detection columns (input features)\n",
    "    for col in [\"rcs\", \"x_det\", \"y_det\", \"radialVelocity\"]:\n",
    "        df[col] = df[col].apply(convert_to_list)\n",
    "\n",
    "    # Apply conversion to label columns (target outputs)\n",
    "    for col in [\"orientation\", \"width_edge_mean\", \"length_edge_mean\", \"centroid_x\", \"centroid_y\"]:\n",
    "        df[col] = df[col].apply(extract_single_value)\n",
    "\n",
    "    # Ensure scalar feature remains float (ego_speed)\n",
    "    df[\"ego_speed\"] = df[\"ego_speed\"].astype(float)\n",
    "\n",
    "# Save cleaned datasets\n",
    "train_df.to_csv(train_path, index=False)\n",
    "val_df.to_csv(val_path, index=False)\n",
    "test_df.to_csv(test_path, index=False)\n",
    "\n",
    "print(\"✅ Input features and labels successfully converted and saved for train, validation, and test sets!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Feature normalization completed and saved!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load datasets\n",
    "train_path = \"/home/q674749/workspace/thesis_work/rat25-15.4.1/perception/BERT_NOTEBOOK/final_leading_object_data/mohan_train.csv\"\n",
    "val_path = \"/home/q674749/workspace/thesis_work/rat25-15.4.1/perception/BERT_NOTEBOOK/final_leading_object_data/mohan_val.csv\"\n",
    "test_path = \"/home/q674749/workspace/thesis_work/rat25-15.4.1/perception/BERT_NOTEBOOK/final_leading_object_data/mohan_test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "val_df = pd.read_csv(val_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Convert list-based detection columns back to lists\n",
    "def convert_to_list(value):\n",
    "    \"\"\"Ensure values are lists. If stored as string, convert back to list of floats.\"\"\"\n",
    "    if isinstance(value, list):\n",
    "        return value\n",
    "    elif isinstance(value, str):\n",
    "        value = value.replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \" \")\n",
    "        return [float(x) for x in value.split() if x.strip()]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Apply conversion to detection columns\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    for col in [\"rcs\", \"x_det\", \"y_det\", \"radialVelocity\"]:\n",
    "        df[col] = df[col].apply(convert_to_list)\n",
    "\n",
    "# Fit scaler on training set only (avoiding data leakage)\n",
    "detection_scaler = StandardScaler()\n",
    "\n",
    "# Flatten all training detections to fit the scaler\n",
    "train_detections = np.vstack([\n",
    "    np.column_stack([\n",
    "        np.array(x), np.array(y), np.array(v), np.array(rcs)\n",
    "    ])\n",
    "    for x, y, v, rcs in zip(train_df[\"x_det\"], train_df[\"y_det\"], train_df[\"radialVelocity\"], train_df[\"rcs\"])\n",
    "])\n",
    "\n",
    "# Fit scaler on training detections\n",
    "detection_scaler.fit(train_detections)\n",
    "\n",
    "# Normalize all datasets\n",
    "def normalize_detections(row):\n",
    "    \"\"\"Normalize detection features while maintaining variable-length structure.\"\"\"\n",
    "    detections = np.column_stack([\n",
    "        row[\"x_det\"], row[\"y_det\"], row[\"radialVelocity\"], row[\"rcs\"]\n",
    "    ])\n",
    "    return detection_scaler.transform(detections).tolist()  # Keep as list for later use\n",
    "\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    df[\"x_det\"] = df.apply(normalize_detections, axis=1)\n",
    "\n",
    "# Normalize ego_speed (fit on train, apply to all)\n",
    "scalar_scaler = StandardScaler()\n",
    "scalar_scaler.fit(train_df[[\"ego_speed\"]])\n",
    "\n",
    "train_df[\"ego_speed\"] = scalar_scaler.transform(train_df[[\"ego_speed\"]])\n",
    "val_df[\"ego_speed\"] = scalar_scaler.transform(val_df[[\"ego_speed\"]])\n",
    "test_df[\"ego_speed\"] = scalar_scaler.transform(test_df[[\"ego_speed\"]])\n",
    "\n",
    "# Save the normalized datasets\n",
    "train_df.to_csv(train_path, index=False)\n",
    "val_df.to_csv(val_path, index=False)\n",
    "test_df.to_csv(test_path, index=False)\n",
    "\n",
    "print(\"✅ Feature normalization completed and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 14:46:12.086700: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TensorFlow datasets created successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the cleaned and normalized datasets\n",
    "train_path = \"/home/q674749/workspace/thesis_work/rat25-15.4.1/perception/BERT_NOTEBOOK/final_leading_object_data/mohan_train.csv\"\n",
    "val_path = \"/home/q674749/workspace/thesis_work/rat25-15.4.1/perception/BERT_NOTEBOOK/final_leading_object_data/mohan_val.csv\"\n",
    "test_path = \"/home/q674749/workspace/thesis_work/rat25-15.4.1/perception/BERT_NOTEBOOK/final_leading_object_data/mohan_test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "val_df = pd.read_csv(val_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Convert detection columns from string to lists\n",
    "def convert_to_list(value):\n",
    "    \"\"\"Convert stored string lists back to actual lists.\"\"\"\n",
    "    if isinstance(value, list):\n",
    "        return value\n",
    "    elif isinstance(value, str):\n",
    "        value = value.replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \" \")\n",
    "        return [float(x) for x in value.split() if x.strip()]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Apply conversion to detection columns\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    for col in [\"rcs\", \"x_det\", \"y_det\", \"radialVelocity\"]:\n",
    "        df[col] = df[col].apply(convert_to_list)\n",
    "\n",
    "# Convert dataset into TensorFlow-compatible format\n",
    "def preprocess(row):\n",
    "    \"\"\"Process a single row into model input format.\"\"\"\n",
    "    # Stack detections into a (N_detections, 4) array\n",
    "    detections = np.column_stack([\n",
    "        row[\"x_det\"],\n",
    "        row[\"y_det\"],\n",
    "        row[\"radialVelocity\"],\n",
    "        row[\"rcs\"]\n",
    "    ])\n",
    "    \n",
    "    # Convert to TensorFlow format (RaggedTensor allows variable-length detections)\n",
    "    detections_tensor = tf.ragged.constant(detections)\n",
    "\n",
    "    # Scalar input: ego_speed\n",
    "    ego_speed_tensor = tf.convert_to_tensor([row[\"ego_speed\"]], dtype=tf.float32)\n",
    "\n",
    "    # Labels\n",
    "    labels = [\n",
    "        row[\"centroid_x\"],\n",
    "        row[\"centroid_y\"],\n",
    "        row[\"orientation\"],\n",
    "        row[\"width_edge_mean\"],\n",
    "        row[\"length_edge_mean\"]\n",
    "    ]\n",
    "    labels_tensor = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "\n",
    "    return {\"detections\": detections_tensor, \"ego_speed\": ego_speed_tensor}, labels_tensor\n",
    "\n",
    "# Convert DataFrame to TensorFlow Dataset\n",
    "def create_dataset(df):\n",
    "    \"\"\"Convert a Pandas DataFrame into a TensorFlow Dataset.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: (preprocess(row) for _, row in df.iterrows()),\n",
    "        output_signature=(\n",
    "            {\n",
    "                \"detections\": tf.RaggedTensorSpec(shape=[None, 4], dtype=tf.float32),\n",
    "                \"ego_speed\": tf.TensorSpec(shape=[1], dtype=tf.float32)\n",
    "            },\n",
    "            tf.TensorSpec(shape=[5], dtype=tf.float32)  # Labels\n",
    "        )\n",
    "    )\n",
    "    return dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Create train, validation, and test datasets\n",
    "train_dataset = create_dataset(train_df)\n",
    "val_dataset = create_dataset(val_df)\n",
    "test_dataset = create_dataset(test_df)\n",
    "\n",
    "print(\"✅ TensorFlow datasets created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"radar_point_net_plus_plus_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_4 (Sequential)   (None, None, 256)         41664     \n",
      "                                                                 \n",
      " lambda_2 (Lambda)           multiple                  0         \n",
      "                                                                 \n",
      " sequential_5 (Sequential)   (None, 5)                 99589     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 141253 (551.77 KB)\n",
      "Trainable params: 141253 (551.77 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class RadarPointNetPlusPlus(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared MLP for per-point feature extraction\n",
    "        self.shared_mlp = tf.keras.Sequential([\n",
    "            Dense(64, activation=\"relu\"),\n",
    "            Dense(128, activation=\"relu\"),\n",
    "            Dense(256, activation=\"relu\")\n",
    "        ])\n",
    "        \n",
    "        # Feature Aggregation (Global Pooling)\n",
    "        self.global_pool = Lambda(lambda x: tf.reduce_max(x, axis=1))\n",
    "\n",
    "        # Fully Connected Layers for Final Prediction\n",
    "        self.fc = tf.keras.Sequential([\n",
    "            Dense(256, activation=\"relu\"),\n",
    "            Dense(128, activation=\"relu\"),\n",
    "            Dense(5)  # Output: [centroid_x, centroid_y, orientation, width, length]\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \"\"\"Forward pass through the model.\"\"\"\n",
    "        detections = inputs[\"detections\"]  # Expected: (batch_size, N_detections, 4)\n",
    "        ego_speed = inputs[\"ego_speed\"]    # Expected: (batch_size, 1)\n",
    "\n",
    "        # Check if detections are RaggedTensor and convert only if necessary\n",
    "        if isinstance(detections, tf.RaggedTensor):\n",
    "            detections = detections.to_tensor()  # Convert to dense tensor only if ragged\n",
    "\n",
    "        # Apply Shared MLP to each detection\n",
    "        point_features = self.shared_mlp(detections)  # Shape: (batch_size, N_detections, 256)\n",
    "\n",
    "        # Feature Aggregation (Max Pooling over detections)\n",
    "        global_features = self.global_pool(point_features)  # Shape: (batch_size, 256)\n",
    "\n",
    "        # Concatenate with ego_speed\n",
    "        combined_features = Concatenate()([global_features, ego_speed])  # Shape: (batch_size, 257)\n",
    "\n",
    "        # Fully Connected Layers for Final Prediction\n",
    "        outputs = self.fc(combined_features)  # Shape: (batch_size, 5)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Create model instance\n",
    "model = RadarPointNetPlusPlus()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "# Fix: Corrected input shape format\n",
    "model.build(input_shape={\"detections\": (None, None, 4), \"ego_speed\": (None, 1)})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new pipeline with removed data bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def preprocess(row):\n",
    "    \"\"\"Process a single row, ensuring all detection features have the same length.\"\"\"\n",
    "    \n",
    "    # Find the minimum length among all detection features in this row\n",
    "    min_len = min(len(row[\"x_det\"]), len(row[\"y_det\"]), len(row[\"radialVelocity\"]), len(row[\"rcs\"]))\n",
    "\n",
    "    # Truncate all lists to ensure they have the same length\n",
    "    x_det = row[\"x_det\"][:min_len]\n",
    "    y_det = row[\"y_det\"][:min_len]\n",
    "    radialVelocity = row[\"radialVelocity\"][:min_len]\n",
    "    rcs = row[\"rcs\"][:min_len]\n",
    "\n",
    "    # Stack detections into a (N_detections, 4) array\n",
    "    detections = np.column_stack([x_det, y_det, radialVelocity, rcs])\n",
    "\n",
    "    # Convert to TensorFlow RaggedTensor\n",
    "    detections_tensor = tf.ragged.constant(detections, dtype=tf.float32)\n",
    "\n",
    "    # Scalar input: ego_speed\n",
    "    ego_speed_tensor = tf.convert_to_tensor([row[\"ego_speed\"]], dtype=tf.float32)\n",
    "\n",
    "    # Labels (Ground truth outputs)\n",
    "    labels = [\n",
    "        row[\"centroid_x\"],\n",
    "        row[\"centroid_y\"],\n",
    "        row[\"orientation\"],\n",
    "        row[\"width_edge_mean\"],\n",
    "        row[\"length_edge_mean\"]\n",
    "    ]\n",
    "    labels_tensor = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "\n",
    "    return {\"detections\": detections_tensor, \"ego_speed\": ego_speed_tensor}, labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TensorFlow datasets created successfully with batch size 32!\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(df, batch_size=32):\n",
    "    \"\"\"Convert a Pandas DataFrame into a TensorFlow Dataset with batching.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: (preprocess(row) for _, row in df.iterrows()),\n",
    "        output_signature=(\n",
    "            {\n",
    "                \"detections\": tf.RaggedTensorSpec(shape=[None, 4], dtype=tf.float32),\n",
    "                \"ego_speed\": tf.TensorSpec(shape=[1], dtype=tf.float32)\n",
    "            },\n",
    "            tf.TensorSpec(shape=[5], dtype=tf.float32)  # Labels: (centroid_x, centroid_y, orientation, width, length)\n",
    "        )\n",
    "    )\n",
    "    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Recreate datasets using the fixed preprocessing function\n",
    "train_dataset = create_dataset(train_df, batch_size=32)\n",
    "val_dataset = create_dataset(val_df, batch_size=32)\n",
    "test_dataset = create_dataset(test_df, batch_size=32)\n",
    "\n",
    "print(\"✅ TensorFlow datasets created successfully with batch size 32!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Shape (detections): (32, None, 4)\n",
      "Batch Shape (ego_speed): (32, 1)\n",
      "Batch Shape (labels): (32, 5)\n"
     ]
    }
   ],
   "source": [
    "for sample in train_dataset.take(1):\n",
    "    print(\"Batch Shape (detections):\", sample[0][\"detections\"].shape)\n",
    "    print(\"Batch Shape (ego_speed):\", sample[0][\"ego_speed\"].shape)\n",
    "    print(\"Batch Shape (labels):\", sample[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TensorFlow session cleared!\n",
      "✅ Model weights have been reset!\n",
      "Epoch 1/50\n",
      "848/848 [==============================] - 627s 737ms/step - loss: 599.7581 - mae: 10.3508 - val_loss: 282.0904 - val_mae: 7.2375\n",
      "Epoch 2/50\n",
      "  1/848 [..............................] - ETA: 38s - loss: 331.5275 - mae: 7.6212WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 42400 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 34\u001b[0m\n\u001b[1;32m     29\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[1;32m     30\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# You can adjust based on performance\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Fix for x/Unknown issue\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Fix for x/Unknown issue\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Plot training and validation loss\u001b[39;00m\n\u001b[1;32m     44\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/engine/training.py:1791\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1776\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[1;32m   1777\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m   1778\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1789\u001b[0m         pss_evaluation_shards\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pss_evaluation_shards,\n\u001b[1;32m   1790\u001b[0m     )\n\u001b[0;32m-> 1791\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1796\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1800\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1802\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1804\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1805\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1806\u001b[0m }\n\u001b[1;32m   1807\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/engine/training.py:2200\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2196\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   2197\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2198\u001b[0m             ):\n\u001b[1;32m   2199\u001b[0m                 callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m-> 2200\u001b[0m                 logs \u001b[38;5;241m=\u001b[39m \u001b[43mtest_function_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2201\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdataset_or_iterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2202\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdata_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2203\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2204\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pss_evaluation_shards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2207\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   2208\u001b[0m \u001b[38;5;66;03m# Override with model metrics instead of last step logs\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/engine/training.py:4000\u001b[0m, in \u001b[0;36m_TestFunction.run_step\u001b[0;34m(self, dataset_or_iterator, data_handler, step, unused_shards)\u001b[0m\n\u001b[1;32m   3999\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_or_iterator, data_handler, step, unused_shards):\n\u001b[0;32m-> 4000\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_or_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4001\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   4002\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:864\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    862\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Optional: Clear TensorFlow session to avoid memory issues\n",
    "K.clear_session()\n",
    "print(\"✅ TensorFlow session cleared!\")\n",
    "\n",
    "# Reinitialize the model to reset weights\n",
    "model = RadarPointNetPlusPlus()\n",
    "\n",
    "# Compile the model again\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "print(\"✅ Model weights have been reset!\")\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Calculate number of batches per epoch\n",
    "steps_per_epoch = len(train_df) // batch_size\n",
    "validation_steps = len(val_df) // batch_size\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,  # You can adjust based on performance\n",
    "    steps_per_epoch=steps_per_epoch,        # Fix for x/Unknown issue\n",
    "    validation_steps=validation_steps,      # Fix for x/Unknown issue\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation MAE\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history[\"mae\"], label=\"Train MAE\")\n",
    "plt.plot(history.history[\"val_mae\"], label=\"Validation MAE\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Mean Absolute Error (MAE)\")\n",
    "plt.title(\"Training and Validation MAE\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows in Dataset: 27155\n",
      "❗ Rows with inconsistent feature lengths: 27155\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file (train dataset as example)\n",
    "csv_path = \"/home/q674749/workspace/thesis_work/rat25-15.4.1/perception/BERT_NOTEBOOK/final_leading_object_data/mohan_train.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert list-based features from string format to lists\n",
    "def convert_to_list(value):\n",
    "    \"\"\"Ensure values are lists. If string, convert to list of floats.\"\"\"\n",
    "    if isinstance(value, list):\n",
    "        return value\n",
    "    elif isinstance(value, str):\n",
    "        value = value.replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \" \")\n",
    "        return [float(x) for x in value.split() if x.strip()]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Apply conversion to detection columns\n",
    "for col in [\"x_det\", \"y_det\", \"radialVelocity\", \"rcs\"]:\n",
    "    df[col] = df[col].apply(convert_to_list)\n",
    "\n",
    "# Check length consistency across rows\n",
    "df[\"x_det_len\"] = df[\"x_det\"].apply(len)\n",
    "df[\"y_det_len\"] = df[\"y_det\"].apply(len)\n",
    "df[\"radialVelocity_len\"] = df[\"radialVelocity\"].apply(len)\n",
    "df[\"rcs_len\"] = df[\"rcs\"].apply(len)\n",
    "\n",
    "# Find rows where lengths are inconsistent\n",
    "inconsistent_rows = df[\n",
    "    (df[\"x_det_len\"] != df[\"y_det_len\"]) |\n",
    "    (df[\"x_det_len\"] != df[\"radialVelocity_len\"]) |\n",
    "    (df[\"x_det_len\"] != df[\"rcs_len\"])\n",
    "]\n",
    "\n",
    "# Print the number of inconsistent rows\n",
    "print(\"Total Rows in Dataset:\", len(df))\n",
    "print(\"❗ Rows with inconsistent feature lengths:\", len(inconsistent_rows))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
