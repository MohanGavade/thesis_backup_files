{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3707bb4f-d339-487a-91dc-b3d65e4c27c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined file saved as: C:\\Python\\WPy64-31241\\notebooks\\bmw_thesis\\combined_file.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Get the current working directory (where the notebook is located)\n",
    "folder_path = os.getcwd()\n",
    "\n",
    "# List to hold all dataframes\n",
    "dfs = []\n",
    "\n",
    "# Loop through all files in the current directory\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".XLSX\") and not file.startswith(\"combined_file\"):  # Skip the combined file if it exists\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_excel(file_path)\n",
    "        dfs.append(df)\n",
    "\n",
    "# Combine all dataframes into one\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Save the combined dataframe to a new Excel file\n",
    "combined_file_path = os.path.join(folder_path, \"combined_file.xlsx\")\n",
    "combined_df.to_excel(combined_file_path, index=False)\n",
    "\n",
    "print(f\"Combined file saved as: {combined_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecc15245-67ea-4c93-b492-74f15b37556e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved output_chunk_1.csv with 16995 rows.\n",
      "Saved output_chunk_2.csv with 15921 rows.\n",
      "Saved output_chunk_3.csv with 14095 rows.\n",
      "Saved output_chunk_4.csv with 13929 rows.\n",
      "Saved output_chunk_5.csv with 13556 rows.\n",
      "Saved output_chunk_6.csv with 13922 rows.\n",
      "Saved output_chunk_7.csv with 13734 rows.\n",
      "Saved output_chunk_8.csv with 13460 rows.\n",
      "Saved output_chunk_9.csv with 14800 rows.\n",
      "Saved output_chunk_10.csv with 15191 rows.\n",
      "Saved output_chunk_11.csv with 15413 rows.\n",
      "Saved output_chunk_12.csv with 15215 rows.\n",
      "Saved output_chunk_13.csv with 14694 rows.\n",
      "Saved output_chunk_14.csv with 16756 rows.\n",
      "Saved output_chunk_15.csv with 15969 rows.\n",
      "Saved output_chunk_16.csv with 15789 rows.\n",
      "Saved output_chunk_17.csv with 15134 rows.\n",
      "Saved output_chunk_18.csv with 14977 rows.\n",
      "Saved output_chunk_19.csv with 17108 rows.\n",
      "Saved output_chunk_20.csv with 15197 rows.\n",
      "Saved output_chunk_21.csv with 14159 rows.\n",
      "Saved output_chunk_22.csv with 13089 rows.\n",
      "Saved output_chunk_23.csv with 13258 rows.\n",
      "Saved output_chunk_24.csv with 14120 rows.\n",
      "Saved output_chunk_25.csv with 15653 rows.\n",
      "Saved output_chunk_26.csv with 14875 rows.\n",
      "Saved output_chunk_27.csv with 15139 rows.\n",
      "Saved output_chunk_28.csv with 13850 rows.\n",
      "Saved output_chunk_29.csv with 15383 rows.\n",
      "Saved output_chunk_30.csv with 14892 rows.\n",
      "Saved output_chunk_31.csv with 15160 rows.\n",
      "Saved output_chunk_32.csv with 14636 rows.\n",
      "Saved output_chunk_33.csv with 16097 rows.\n",
      "Saved output_chunk_34.csv with 17034 rows.\n",
      "Saved output_chunk_35.csv with 16505 rows.\n",
      "Saved output_chunk_36.csv with 16251 rows.\n",
      "Saved output_chunk_37.csv with 15589 rows.\n",
      "Saved output_chunk_38.csv with 17407 rows.\n",
      "Saved output_chunk_39.csv with 14170 rows.\n",
      "Saved output_chunk_40.csv with 14370 rows.\n",
      "Saved output_chunk_41.csv with 15765 rows.\n",
      "Saved output_chunk_42.csv with 16003 rows.\n",
      "Saved output_chunk_43.csv with 16269 rows.\n",
      "Saved output_chunk_44.csv with 17774 rows.\n",
      "Saved output_chunk_45.csv with 14950 rows.\n",
      "Saved output_chunk_46.csv with 15907 rows.\n",
      "Saved output_chunk_47.csv with 16042 rows.\n",
      "Saved output_chunk_48.csv with 15969 rows.\n",
      "Saved output_chunk_49.csv with 16741 rows.\n",
      "Saved output_chunk_50.csv with 15869 rows.\n",
      "Saved output_chunk_51.csv with 15736 rows.\n",
      "Saved output_chunk_52.csv with 17824 rows.\n",
      "Saved output_chunk_53.csv with 15895 rows.\n",
      "Saved output_chunk_54.csv with 14049 rows.\n",
      "Saved output_chunk_55.csv with 13990 rows.\n",
      "Saved output_chunk_56.csv with 14974 rows.\n",
      "Saved output_chunk_57.csv with 15372 rows.\n",
      "Saved output_chunk_58.csv with 16049 rows.\n",
      "Saved output_chunk_59.csv with 15825 rows.\n",
      "Saved output_chunk_60.csv with 15670 rows.\n",
      "Saved output_chunk_61.csv with 15622 rows.\n",
      "Saved output_chunk_62.csv with 15827 rows.\n",
      "Saved output_chunk_63.csv with 15545 rows.\n",
      "Saved output_chunk_64.csv with 15490 rows.\n",
      "Saved output_chunk_65.csv with 15764 rows.\n",
      "Saved output_chunk_66.csv with 15688 rows.\n",
      "Saved output_chunk_67.csv with 16032 rows.\n",
      "Saved output_chunk_68.csv with 17358 rows.\n",
      "Saved output_chunk_69.csv with 16237 rows.\n",
      "Saved output_chunk_70.csv with 17309 rows.\n",
      "Saved output_chunk_71.csv with 19564 rows.\n",
      "Saved output_chunk_72.csv with 17392 rows.\n",
      "Saved output_chunk_73.csv with 14271 rows.\n",
      "Saved output_chunk_74.csv with 15781 rows.\n",
      "Saved output_chunk_75.csv with 19015 rows.\n",
      "Saved output_chunk_76.csv with 13416 rows.\n",
      "Saved output_chunk_77.csv with 15848 rows.\n",
      "Saved output_chunk_78.csv with 13863 rows.\n",
      "Saved output_chunk_79.csv with 15927 rows.\n",
      "Saved output_chunk_80.csv with 16742 rows.\n",
      "Saved output_chunk_81.csv with 14404 rows.\n",
      "Saved output_chunk_82.csv with 16719 rows.\n",
      "Saved output_chunk_83.csv with 15920 rows.\n",
      "Saved output_chunk_84.csv with 15036 rows.\n",
      "Saved output_chunk_85.csv with 16315 rows.\n",
      "Saved output_chunk_86.csv with 14589 rows.\n",
      "Saved output_chunk_87.csv with 15141 rows.\n",
      "Saved output_chunk_88.csv with 16250 rows.\n",
      "Saved output_chunk_89.csv with 15621 rows.\n",
      "Saved output_chunk_90.csv with 15698 rows.\n",
      "Saved output_chunk_91.csv with 15213 rows.\n",
      "Saved output_chunk_92.csv with 16268 rows.\n",
      "Saved output_chunk_93.csv with 16326 rows.\n",
      "Saved output_chunk_94.csv with 19211 rows.\n",
      "Saved output_chunk_95.csv with 18223 rows.\n",
      "Saved output_chunk_96.csv with 15289 rows.\n",
      "Saved output_chunk_97.csv with 15408 rows.\n",
      "Saved output_chunk_98.csv with 15804 rows.\n",
      "Saved output_chunk_99.csv with 17365 rows.\n",
      "Saved output_chunk_100.csv with 14881 rows.\n",
      "Saved output_chunk_101.csv with 16119 rows.\n",
      "Saved output_chunk_102.csv with 16621 rows.\n",
      "Saved output_chunk_103.csv with 15291 rows.\n",
      "Saved output_chunk_104.csv with 15192 rows.\n",
      "Saved output_chunk_105.csv with 13118 rows.\n",
      "Saved output_chunk_106.csv with 14049 rows.\n",
      "Saved output_chunk_107.csv with 15291 rows.\n",
      "Saved output_chunk_108.csv with 15155 rows.\n",
      "Saved output_chunk_109.csv with 15599 rows.\n",
      "Saved output_chunk_110.csv with 17260 rows.\n",
      "Saved output_chunk_111.csv with 13483 rows.\n",
      "Saved output_chunk_112.csv with 15373 rows.\n",
      "Saved output_chunk_113.csv with 17302 rows.\n",
      "Saved output_chunk_114.csv with 16777 rows.\n",
      "Saved output_chunk_115.csv with 14716 rows.\n",
      "Saved output_chunk_116.csv with 15726 rows.\n",
      "Saved output_chunk_117.csv with 16086 rows.\n",
      "Saved output_chunk_118.csv with 16075 rows.\n",
      "Saved output_chunk_119.csv with 16350 rows.\n",
      "Saved output_chunk_120.csv with 16877 rows.\n",
      "Saved output_chunk_121.csv with 15601 rows.\n",
      "Saved output_chunk_122.csv with 16055 rows.\n",
      "Saved output_chunk_123.csv with 16288 rows.\n",
      "Saved output_chunk_124.csv with 16897 rows.\n",
      "Saved output_chunk_125.csv with 15714 rows.\n",
      "Saved output_chunk_126.csv with 16877 rows.\n",
      "Saved output_chunk_127.csv with 13695 rows.\n",
      "Saved output_chunk_128.csv with 13319 rows.\n",
      "Saved output_chunk_129.csv with 16273 rows.\n",
      "Saved output_chunk_130.csv with 14076 rows.\n",
      "Saved output_chunk_131.csv with 16620 rows.\n",
      "Saved output_chunk_132.csv with 15536 rows.\n",
      "Saved output_chunk_133.csv with 14155 rows.\n",
      "Saved output_chunk_134.csv with 16753 rows.\n",
      "Saved output_chunk_135.csv with 17048 rows.\n",
      "Saved output_chunk_136.csv with 16663 rows.\n",
      "Saved output_chunk_137.csv with 16870 rows.\n",
      "Saved output_chunk_138.csv with 16347 rows.\n",
      "Saved output_chunk_139.csv with 15220 rows.\n",
      "Saved output_chunk_140.csv with 18233 rows.\n",
      "Saved output_chunk_141.csv with 14302 rows.\n",
      "Saved output_chunk_142.csv with 16206 rows.\n",
      "Saved output_chunk_143.csv with 16214 rows.\n",
      "Saved output_chunk_144.csv with 15263 rows.\n",
      "Saved output_chunk_145.csv with 16304 rows.\n",
      "Saved output_chunk_146.csv with 42434 rows.\n",
      "Saved output_chunk_147.csv with 15983 rows.\n",
      "Saved output_chunk_148.csv with 15291 rows.\n",
      "Saved output_chunk_149.csv with 14650 rows.\n",
      "Saved output_chunk_150.csv with 15832 rows.\n",
      "Saved output_chunk_151.csv with 15606 rows.\n",
      "Saved output_chunk_152.csv with 15616 rows.\n",
      "Saved output_chunk_153.csv with 13944 rows.\n",
      "Saved output_chunk_154.csv with 15070 rows.\n",
      "Saved output_chunk_155.csv with 15180 rows.\n",
      "Saved output_chunk_156.csv with 16070 rows.\n",
      "Saved output_chunk_157.csv with 15116 rows.\n",
      "Saved output_chunk_158.csv with 15075 rows.\n",
      "Saved output_chunk_159.csv with 14825 rows.\n",
      "Saved output_chunk_160.csv with 14149 rows.\n",
      "Saved output_chunk_161.csv with 16023 rows.\n",
      "Saved output_chunk_162.csv with 14786 rows.\n",
      "Saved output_chunk_163.csv with 14870 rows.\n",
      "Saved output_chunk_164.csv with 15361 rows.\n",
      "Saved output_chunk_165.csv with 15719 rows.\n",
      "Saved output_chunk_166.csv with 15453 rows.\n",
      "Saved output_chunk_167.csv with 15831 rows.\n",
      "Saved output_chunk_168.csv with 15835 rows.\n",
      "Saved output_chunk_169.csv with 15283 rows.\n",
      "Saved output_chunk_170.csv with 15712 rows.\n",
      "Saved output_chunk_171.csv with 13757 rows.\n",
      "Saved output_chunk_172.csv with 15593 rows.\n",
      "Saved output_chunk_173.csv with 13342 rows.\n",
      "Saved output_chunk_174.csv with 14217 rows.\n",
      "Saved output_chunk_175.csv with 15045 rows.\n",
      "Saved output_chunk_176.csv with 15946 rows.\n",
      "Saved output_chunk_177.csv with 16192 rows.\n",
      "Saved output_chunk_178.csv with 15941 rows.\n",
      "Saved output_chunk_179.csv with 16247 rows.\n",
      "Saved output_chunk_180.csv with 16719 rows.\n",
      "Saved output_chunk_181.csv with 14431 rows.\n",
      "Saved output_chunk_182.csv with 13996 rows.\n",
      "Saved output_chunk_183.csv with 13909 rows.\n",
      "Saved output_chunk_184.csv with 15813 rows.\n",
      "Saved output_chunk_185.csv with 16861 rows.\n",
      "Saved output_chunk_186.csv with 15426 rows.\n",
      "Saved output_chunk_187.csv with 15786 rows.\n",
      "Saved output_chunk_188.csv with 31349 rows.\n",
      "Saved output_chunk_189.csv with 16246 rows.\n",
      "Saved output_chunk_190.csv with 15006 rows.\n",
      "Saved output_chunk_191.csv with 15119 rows.\n",
      "Saved output_chunk_192.csv with 15306 rows.\n",
      "Saved output_chunk_193.csv with 15797 rows.\n",
      "Saved output_chunk_194.csv with 15979 rows.\n",
      "Saved output_chunk_195.csv with 15253 rows.\n",
      "Saved output_chunk_196.csv with 15402 rows.\n",
      "Saved output_chunk_197.csv with 14863 rows.\n",
      "Saved output_chunk_198.csv with 16301 rows.\n",
      "Saved output_chunk_199.csv with 13740 rows.\n",
      "Saved output_chunk_200.csv with 17046 rows.\n",
      "Saved output_chunk_201.csv with 16237 rows.\n",
      "Saved output_chunk_202.csv with 15935 rows.\n",
      "Saved output_chunk_203.csv with 15889 rows.\n",
      "Saved output_chunk_204.csv with 15380 rows.\n",
      "Saved output_chunk_205.csv with 15185 rows.\n",
      "Saved output_chunk_206.csv with 14278 rows.\n",
      "Saved output_chunk_207.csv with 14887 rows.\n",
      "Saved output_chunk_208.csv with 14709 rows.\n",
      "Saved output_chunk_209.csv with 15295 rows.\n",
      "Saved output_chunk_210.csv with 15944 rows.\n",
      "Saved output_chunk_211.csv with 16351 rows.\n",
      "Saved output_chunk_212.csv with 16166 rows.\n",
      "Saved output_chunk_213.csv with 16352 rows.\n",
      "Saved output_chunk_214.csv with 15992 rows.\n",
      "Saved output_chunk_215.csv with 16534 rows.\n",
      "Saved output_chunk_216.csv with 16035 rows.\n",
      "Saved output_chunk_217.csv with 15476 rows.\n",
      "Saved output_chunk_218.csv with 14035 rows.\n",
      "Saved output_chunk_219.csv with 15050 rows.\n",
      "Saved output_chunk_220.csv with 15159 rows.\n",
      "Saved output_chunk_221.csv with 14402 rows.\n",
      "Saved output_chunk_222.csv with 17864 rows.\n",
      "Saved output_chunk_223.csv with 15754 rows.\n",
      "Saved output_chunk_224.csv with 15943 rows.\n",
      "Saved output_chunk_225.csv with 15746 rows.\n",
      "Saved output_chunk_226.csv with 14628 rows.\n",
      "Saved output_chunk_227.csv with 14193 rows.\n",
      "Saved output_chunk_228.csv with 14051 rows.\n",
      "Saved output_chunk_229.csv with 14225 rows.\n",
      "Saved output_chunk_230.csv with 14452 rows.\n",
      "Saved output_chunk_231.csv with 15465 rows.\n",
      "Saved output_chunk_232.csv with 14006 rows.\n",
      "Saved output_chunk_233.csv with 15546 rows.\n",
      "Saved output_chunk_234.csv with 16884 rows.\n",
      "Saved output_chunk_235.csv with 15047 rows.\n",
      "Saved output_chunk_236.csv with 15743 rows.\n",
      "Saved output_chunk_237.csv with 14903 rows.\n",
      "Saved output_chunk_238.csv with 15380 rows.\n",
      "Saved output_chunk_239.csv with 16222 rows.\n",
      "Saved output_chunk_240.csv with 15608 rows.\n",
      "Saved output_chunk_241.csv with 38482 rows.\n",
      "Saved output_chunk_242.csv with 14410 rows.\n",
      "Saved output_chunk_243.csv with 15430 rows.\n",
      "Saved output_chunk_244.csv with 16746 rows.\n",
      "Saved output_chunk_245.csv with 15628 rows.\n",
      "Saved output_chunk_246.csv with 15728 rows.\n",
      "Saved output_chunk_247.csv with 15119 rows.\n",
      "Saved output_chunk_248.csv with 15228 rows.\n",
      "Saved output_chunk_249.csv with 16355 rows.\n",
      "Saved output_chunk_250.csv with 16080 rows.\n",
      "Saved output_chunk_251.csv with 14135 rows.\n",
      "Saved output_chunk_252.csv with 16659 rows.\n",
      "Saved output_chunk_253.csv with 15519 rows.\n",
      "Saved output_chunk_254.csv with 15193 rows.\n",
      "Saved output_chunk_255.csv with 14542 rows.\n",
      "Saved output_chunk_256.csv with 13783 rows.\n",
      "Saved output_chunk_257.csv with 16357 rows.\n",
      "Saved output_chunk_258.csv with 15217 rows.\n",
      "Saved output_chunk_259.csv with 14344 rows.\n",
      "Saved output_chunk_260.csv with 15197 rows.\n",
      "Saved output_chunk_261.csv with 14894 rows.\n",
      "Saved output_chunk_262.csv with 16842 rows.\n",
      "Saved output_chunk_263.csv with 14594 rows.\n",
      "Saved output_chunk_264.csv with 15711 rows.\n",
      "Saved output_chunk_265.csv with 14897 rows.\n",
      "Saved output_chunk_266.csv with 14698 rows.\n",
      "Saved output_chunk_267.csv with 17592 rows.\n",
      "Saved output_chunk_268.csv with 17602 rows.\n",
      "Saved output_chunk_269.csv with 14091 rows.\n",
      "Saved output_chunk_270.csv with 14524 rows.\n",
      "Saved output_chunk_271.csv with 15790 rows.\n",
      "Saved output_chunk_272.csv with 14260 rows.\n",
      "Saved output_chunk_273.csv with 14820 rows.\n",
      "Saved output_chunk_274.csv with 16166 rows.\n",
      "Saved output_chunk_275.csv with 15528 rows.\n",
      "Saved output_chunk_276.csv with 15699 rows.\n",
      "Saved output_chunk_277.csv with 16484 rows.\n",
      "Saved output_chunk_278.csv with 15106 rows.\n",
      "Saved output_chunk_279.csv with 15439 rows.\n",
      "Saved output_chunk_280.csv with 15392 rows.\n",
      "Saved output_chunk_281.csv with 14462 rows.\n",
      "Saved output_chunk_282.csv with 15843 rows.\n",
      "Saved output_chunk_283.csv with 15044 rows.\n",
      "Saved output_chunk_284.csv with 15831 rows.\n",
      "Saved output_chunk_285.csv with 15624 rows.\n",
      "Saved output_chunk_286.csv with 15317 rows.\n",
      "Saved output_chunk_287.csv with 18884 rows.\n",
      "Saved output_chunk_288.csv with 15573 rows.\n",
      "Saved output_chunk_289.csv with 14083 rows.\n",
      "Saved output_chunk_290.csv with 14951 rows.\n",
      "Saved output_chunk_291.csv with 15984 rows.\n",
      "Saved output_chunk_292.csv with 15810 rows.\n",
      "Saved output_chunk_293.csv with 13903 rows.\n",
      "Saved output_chunk_294.csv with 14413 rows.\n",
      "Saved output_chunk_295.csv with 15496 rows.\n",
      "Saved output_chunk_296.csv with 15597 rows.\n",
      "Saved output_chunk_297.csv with 14010 rows.\n",
      "Saved output_chunk_298.csv with 15840 rows.\n",
      "Saved output_chunk_299.csv with 15948 rows.\n",
      "Saved output_chunk_300.csv with 15566 rows.\n",
      "Saved output_chunk_301.csv with 14965 rows.\n",
      "Saved output_chunk_302.csv with 15071 rows.\n",
      "Saved output_chunk_303.csv with 15662 rows.\n",
      "Saved output_chunk_304.csv with 15248 rows.\n",
      "Saved output_chunk_305.csv with 14881 rows.\n",
      "Saved output_chunk_306.csv with 15345 rows.\n",
      "Saved output_chunk_307.csv with 15127 rows.\n",
      "Saved output_chunk_308.csv with 14667 rows.\n",
      "Saved output_chunk_309.csv with 13957 rows.\n",
      "Saved output_chunk_310.csv with 13535 rows.\n",
      "Saved output_chunk_311.csv with 14072 rows.\n",
      "Saved output_chunk_312.csv with 14041 rows.\n",
      "Saved output_chunk_313.csv with 14635 rows.\n",
      "Saved output_chunk_314.csv with 13939 rows.\n",
      "Saved output_chunk_315.csv with 14394 rows.\n",
      "Saved output_chunk_316.csv with 15069 rows.\n",
      "Saved output_chunk_317.csv with 13998 rows.\n",
      "Saved output_chunk_318.csv with 13509 rows.\n",
      "Saved output_chunk_319.csv with 14354 rows.\n",
      "Saved output_chunk_320.csv with 13748 rows.\n",
      "Saved output_chunk_321.csv with 13832 rows.\n",
      "Saved output_chunk_322.csv with 14215 rows.\n",
      "Saved output_chunk_323.csv with 14244 rows.\n",
      "Saved output_chunk_324.csv with 14778 rows.\n",
      "Saved output_chunk_325.csv with 14231 rows.\n",
      "Saved output_chunk_326.csv with 14359 rows.\n",
      "Saved output_chunk_327.csv with 14065 rows.\n",
      "Saved output_chunk_328.csv with 14893 rows.\n",
      "Saved output_chunk_329.csv with 14260 rows.\n",
      "Saved output_chunk_330.csv with 14250 rows.\n",
      "Saved output_chunk_331.csv with 14068 rows.\n",
      "Saved output_chunk_332.csv with 14237 rows.\n",
      "Saved output_chunk_333.csv with 15447 rows.\n",
      "Saved output_chunk_334.csv with 13889 rows.\n",
      "Saved output_chunk_335.csv with 13565 rows.\n",
      "Saved output_chunk_336.csv with 14126 rows.\n",
      "Saved output_chunk_337.csv with 15139 rows.\n",
      "Saved output_chunk_338.csv with 15188 rows.\n",
      "Saved output_chunk_339.csv with 14493 rows.\n",
      "Saved output_chunk_340.csv with 15424 rows.\n",
      "Saved output_chunk_341.csv with 14883 rows.\n",
      "Saved output_chunk_342.csv with 14633 rows.\n",
      "Saved output_chunk_343.csv with 14489 rows.\n",
      "Saved output_chunk_344.csv with 14454 rows.\n",
      "Saved output_chunk_345.csv with 14182 rows.\n",
      "Saved output_chunk_346.csv with 14661 rows.\n",
      "Saved output_chunk_347.csv with 16337 rows.\n",
      "Saved output_chunk_348.csv with 14305 rows.\n",
      "Saved output_chunk_349.csv with 14163 rows.\n",
      "Saved output_chunk_350.csv with 14147 rows.\n",
      "Saved output_chunk_351.csv with 14822 rows.\n",
      "Saved output_chunk_352.csv with 16360 rows.\n",
      "Saved output_chunk_353.csv with 15526 rows.\n",
      "Saved output_chunk_354.csv with 13365 rows.\n",
      "Saved output_chunk_355.csv with 15113 rows.\n",
      "Saved output_chunk_356.csv with 15928 rows.\n",
      "Saved output_chunk_357.csv with 13826 rows.\n",
      "Saved output_chunk_358.csv with 14393 rows.\n",
      "Saved output_chunk_359.csv with 14693 rows.\n",
      "Saved output_chunk_360.csv with 15574 rows.\n",
      "Saved output_chunk_361.csv with 15865 rows.\n",
      "Saved output_chunk_362.csv with 14901 rows.\n",
      "Saved output_chunk_363.csv with 15143 rows.\n",
      "Saved output_chunk_364.csv with 14813 rows.\n",
      "Saved output_chunk_365.csv with 16041 rows.\n",
      "Saved output_chunk_366.csv with 17115 rows.\n",
      "Saved output_chunk_367.csv with 15813 rows.\n",
      "Saved output_chunk_368.csv with 15446 rows.\n",
      "Saved output_chunk_369.csv with 15693 rows.\n",
      "Saved output_chunk_370.csv with 16318 rows.\n",
      "Saved output_chunk_371.csv with 13819 rows.\n",
      "Saved output_chunk_372.csv with 14948 rows.\n",
      "Saved output_chunk_373.csv with 15191 rows.\n",
      "Saved output_chunk_374.csv with 14305 rows.\n",
      "Saved output_chunk_375.csv with 13378 rows.\n",
      "Saved output_chunk_376.csv with 16031 rows.\n",
      "Saved output_chunk_377.csv with 15272 rows.\n",
      "Saved output_chunk_378.csv with 10090 rows.\n",
      "Saved output_chunk_379.csv with 985 rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "input_file = \"Protocol.csv\"  # Replace with your file path\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Get unique IDENTNR values\n",
    "unique_idents = df['IDENTNR'].unique()\n",
    "\n",
    "# Split into chunks of 20 IDENTNR values\n",
    "chunk_size = 8\n",
    "for i in range(0, len(unique_idents), chunk_size):\n",
    "    # Get the current chunk of IDENTNR values\n",
    "    chunk_idents = unique_idents[i:i + chunk_size]\n",
    "    \n",
    "    # Filter the DataFrame for the current chunk\n",
    "    chunk_df = df[df['IDENTNR'].isin(chunk_idents)]\n",
    "    \n",
    "    # Save the chunk to a new CSV file\n",
    "    output_file = f\"output_chunk_{i//chunk_size + 1}.csv\"  # File name like output_chunk_1.csv, output_chunk_2.csv, etc.\n",
    "    chunk_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Saved {output_file} with {len(chunk_df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e944f63-941f-480e-99b3-12580cf837fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e40b97-ab11-4d55-98c3-51d7ea505834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360dfe9a-5fce-4919-9169-23bb8a027322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975790cb-b50c-4954-9fbf-dc416056496b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d8c50d0-b5c8-4b8b-9bbe-329e7e6614dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the steps as functions for modularity and reusability\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load the dataset from a file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)  # Assuming Excel file\n",
    "        print(f\"Data loaded successfully from {file_path}!\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data from {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def filter_data(df):\n",
    "    \"\"\"\n",
    "    Filter the dataset based on the conditions:\n",
    "    - Include all records where Grund der Anfrage is \"E\", \"G\", or \"I\".\n",
    "    - For other records, include only if Antwortstatus is \"APPROVED\" and Anforderungsstatus is \"RECEIVED\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        condition1 = df['Grund der Anfrage'].isin(['E', 'G', 'I'])\n",
    "        condition2 = (df['Antwortstatus'] == 'APPROVED') & (df['Anforderungsstatus'] == 'RECEIVED')\n",
    "        filtered_df = df[condition1 | condition2]\n",
    "        print(\"Data filtered successfully!\")\n",
    "        return filtered_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error filtering data: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def summarize_data(df, agreement_columns):\n",
    "    \"\"\"\n",
    "    Summarize the data by Produktnummer.\n",
    "    - Concatenate Firmenname.\n",
    "    - Keep the first occurrence of each agreement column.\n",
    "    - Include Produktkurztext.\n",
    "    - Exclude Grund der Anfrage and Anforderungsnr. if they are inconsistent.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define aggregation rules for each column\n",
    "        agg_rules = {\n",
    "            'Firmenname': lambda x: ', '.join(x.astype(str)),\n",
    "            'Produktkurztext': 'first',  # Include Produktkurztext\n",
    "            'Grund der Anfrage': lambda x: x.iloc[0] if x.nunique() == 1 else 'MULTIPLE',\n",
    "            'Anforderungsnr.': lambda x: x.iloc[0] if x.nunique() == 1 else 'MULTIPLE'\n",
    "        }\n",
    "\n",
    "        # Add agreement columns to the aggregation rules\n",
    "        for col in agreement_columns:\n",
    "            agg_rules[col] = 'first'\n",
    "\n",
    "        # Group by Produktnummer and aggregate fields\n",
    "        summarized_df = df.groupby('Produktnummer').agg(agg_rules).reset_index()\n",
    "\n",
    "        print(\"Data summarized successfully!\")\n",
    "        return summarized_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error summarizing data: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def modify_fields(df, agreement_columns):\n",
    "    \"\"\"\n",
    "    Rename agreement columns to the desired format.\n",
    "    Add ZEU1_EGIPREF fields based on each agreement column.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Mapping of agreement columns to their new names\n",
    "        agreement_mapping = {\n",
    "            \"EUAND - Präf. Krit.\": \"ZEU3_PREF\",\n",
    "            \"EUCAM - Präf. Krit.\": \"ZEU10_PREF\",\n",
    "            \"EUCET - Präf. Krit.\": \"ZEU5_PREF\",\n",
    "            \"EUCAR - Präf. Krit.\": \"ZEU4_PREF\",\n",
    "            \"EUCHI - Präf. Krit.\": \"ZEU8_PREF\",\n",
    "            \"EUGB - Präf. Krit.\": \"ZEU12_PREF\",\n",
    "            \"EUKOR - Präf. Krit.\": \"ZEU2_PREF\",\n",
    "            \"EUMED - Präf. Krit.\": \"ZEU1_PREF\",\n",
    "            \"EUMEX - Präf. Krit.\": \"ZEU7_PREF\",\n",
    "            \"EUSAD - Präf. Krit.\": \"ZEU9_PREF\",\n",
    "            \"EUUKR - Präf. Krit.\": \"ZEU6_PREF\",\n",
    "            \"EUASE - Präf. Krit.\": \"ZEU11_PREF\",\n",
    "            \"EUPEM - Präf. Krit.\": \"ZEU13_PREF\"\n",
    "        }\n",
    "\n",
    "        # Rename agreement columns\n",
    "        for old_col, new_col in agreement_mapping.items():\n",
    "            if old_col in df.columns:\n",
    "                df = df.rename(columns={old_col: new_col})\n",
    "            else:\n",
    "                print(f\"Warning: Column '{old_col}' not found in the dataset.\")\n",
    "\n",
    "        print(\"Fields modified successfully!\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error modifying fields: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_output(df, output_file):\n",
    "    \"\"\"Save the final output to a CSV file.\"\"\"\n",
    "    try:\n",
    "        df.to_excel(output_file, index=False)\n",
    "        print(f\"Output saved to {output_file}!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving output: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Define the input file and agreement columns\n",
    "    input_file = \"combined_file.xlsx\"  # Replace with your file path\n",
    "    output_file = \"final_output.xlsx\"  # Single output file\n",
    "    agreement_columns = [\n",
    "        \"EUAND - Präf. Krit.\", \"EUASE - Präf. Krit.\", \"EUCAM - Präf. Krit.\", \n",
    "        \"EUCAR - Präf. Krit.\", \"EUCET - Präf. Krit.\", \"EUCHI - Präf. Krit.\", \n",
    "        \"EUGB - Präf. Krit.\", \"EUKOR - Präf. Krit.\", \"EUMED - Präf. Krit.\", \n",
    "        \"EUMEX - Präf. Krit.\", \"EUSAD - Präf. Krit.\", \"EUUKR - Präf. Krit.\", \n",
    "        \"EUPEM - Präf. Krit.\"\n",
    "    ]\n",
    "\n",
    "    # Load the data\n",
    "    df = load_data(input_file)\n",
    "    if df is None:\n",
    "        return\n",
    "\n",
    "    # Filter the data\n",
    "    df = filter_data(df)\n",
    "    if df is None:\n",
    "        return\n",
    "\n",
    "    # Summarize the data\n",
    "    df = summarize_data(df, agreement_columns)\n",
    "    if df is None:\n",
    "        return\n",
    "\n",
    "    # Modify fields\n",
    "    df = modify_fields(df, agreement_columns)\n",
    "    if df is None:\n",
    "        return\n",
    "\n",
    "    # Save the output\n",
    "    save_output(df, output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ceab5402-c5d5-4be2-aa47-06d81cb7ac55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from combined_file.xlsx!\n",
      "Data filtered successfully!\n",
      "Data summarized successfully!\n",
      "Fields modified successfully!\n",
      "Output saved to final_output.xlsx!\n"
     ]
    }
   ],
   "source": [
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b48cf9-726d-457c-b719-c4bca1c0387d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91be16a-8712-4c86-bc04-24bdf9af9be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98574e3-69f8-4ff7-b3dd-062d4ed9110a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d88744d2-b01f-4456-93f8-b48a9cafe6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def sort_database(df):\n",
    "    \"\"\"\n",
    "    Sort the data by:\n",
    "    - \"Datum Präferenzkalkulation\" (descending)\n",
    "    - \"Produktnummer\" (descending)\n",
    "    - \"Abkommen\" (ascending)\n",
    "    - \"Kennzeichen RWPK / IPK\" (descending)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sorted_df = df.sort_values(\n",
    "            by=[\n",
    "                \"Datum Präferenzkalkulation\", \n",
    "                \"Produktnummer\", \n",
    "                \"Abkommen\", \n",
    "                \"Kennzeichen RWPK / IPK\"\n",
    "            ],\n",
    "            ascending=[False, False, True, False]  # D=Descending, A=Ascending in VBA\n",
    "        )\n",
    "        print(\"Data sorted successfully!\")\n",
    "        return sorted_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error sorting data: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def summarization(df):\n",
    "    \"\"\"\n",
    "    Remove duplicates based on \"Produktnummer\" and \"Abkommen\".\n",
    "    Keep the first occurrence (oldest entry if sorted by date descending).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        summarized_df = df.drop_duplicates(\n",
    "            subset=[\"Produktnummer\", \"Abkommen\"], \n",
    "            keep=\"first\"  # Keep the first occurrence (most recent date due to sorting)\n",
    "        )\n",
    "        print(\"Duplicates removed successfully!\")\n",
    "        return summarized_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error removing duplicates: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def join_database(original_df, summarized_df):\n",
    "    \"\"\"\n",
    "    Join the summarized data with the original data to retain only non-duplicate rows.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Columns to match for the join\n",
    "        join_columns = [\n",
    "            \"Produktnummer\", \n",
    "            \"Datum Präferenzkalkulation\", \n",
    "            \"Abkommen\", \n",
    "            \"Kennzeichen RWPK / IPK\"\n",
    "        ]\n",
    "        \n",
    "        # Inner join to keep only matching rows (non-duplicates)\n",
    "        joined_df = pd.merge(\n",
    "            original_df,\n",
    "            summarized_df[join_columns],\n",
    "            on=join_columns,\n",
    "            how=\"inner\"\n",
    "        )\n",
    "        print(\"Data joined successfully!\")\n",
    "        return joined_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error joining data: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    input_file = \"combined_file.xlsx\"  # Replace with your file path\n",
    "    output_file = \"final_output.xlsx\"  # Output file path\n",
    "\n",
    "    try:\n",
    "        df = pd.read_excel(input_file)\n",
    "        print(\"Data loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return\n",
    "\n",
    "    # Step 1: Sort the data\n",
    "    sorted_df = sort_database(df)\n",
    "    if sorted_df is None:\n",
    "        return\n",
    "\n",
    "    # Step 2: Remove duplicates\n",
    "    summarized_df = summarization(sorted_df)\n",
    "    if summarized_df is None:\n",
    "        return\n",
    "\n",
    "    # Step 3: Join to retain non-duplicates\n",
    "    final_df = join_database(df, summarized_df)\n",
    "    if final_df is None:\n",
    "        return\n",
    "\n",
    "    # Save the final output\n",
    "    try:\n",
    "        final_df.to_excel(output_file, index=False)\n",
    "        print(f\"Output saved to {output_file}!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving output: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0712e8ec-fc72-4217-9d47-7ea14cd027a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "Data sorted successfully!\n",
      "Duplicates removed successfully!\n",
      "Data joined successfully!\n",
      "Output saved to final_output.xlsx!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c80cd1b-e19e-4d7a-a960-43bf1741b15c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
